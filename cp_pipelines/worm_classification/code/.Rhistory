knitr::opts_chunk$set(echo = TRUE, fig.width = 16, fig.height = 8, message = FALSE, warning = FALSE)
library(tidymodels)
library(tidymodels)
library(tidyverse)
library(here)
files <- tibble(path = list.files(path = here(),
)
''
''
files <- tibble(path = list.files(path = here(),
pattern = '.*_manual.csv'))
View(files)
files <- tibble(path = list.files(path = here(),
pattern = '.*_manual.csv',
recursive = TRUE))
files <- tibble(path = list.files(path = here(),
pattern = '.*_manual.csv',
recursive = TRUE))
here()
knitr::opts_chunk$set(echo = TRUE, fig.width = 16, fig.height = 8, message = FALSE, warning = FALSE)
library(tidymodels)
library(tidyverse)
library(here)
files <- tibble(path = list.files(path = here(),
pattern = '.*_manual.csv',
recursive = TRUE))
annotations <- files %>%
pmap_dfr(path, read_csv)
View(files)
annotations <- files %>%
pmap_dfr(path, read_csv(here()))
annotations <- files %>%
pmap_dfr(read_csv)
get_data <- function(...) {
df <- tibble(...)
data <- read_csv(here(df$path))
}
annotations <- files %>%
pmap_dfr(get_data)
annotations <- files %>%
pmap_dfr(get_data) %>%
janitor::clean_names() %>%
filter(!is.na(worm)) %>%
select(worm, contains('area')) %>%
mutate(worm = as.factor(worm))
knitr::opts_chunk$set(echo = TRUE, fig.width = 16, fig.height = 8, message = FALSE, warning = FALSE)
library(tidymodels)
library(tidyverse)
library(here)
files <- tibble(path = list.files(path = here(),
pattern = '.*_manual.csv',
recursive = TRUE))
annotations <- files %>%
pmap_dfr(get_data) %>%
janitor::clean_names() %>%
filter(!is.na(worm)) %>%
select(worm, contains('area')) %>%
mutate(worm = case_when(
worm == 'Y' ~ 'Single worm',
worm == 'N' ~ 'Debris',
worm == 'P' ~ 'Partial worm',
worm == 'M' ~ 'Multiple worms',
)) %>%
mutate(worm = as.factor(worm))
get_data <- function(...) {
df <- tibble(...)
data <- read_csv(here(df$path))
}
annotations <- files %>%
pmap_dfr(get_data) %>%
janitor::clean_names() %>%
filter(!is.na(worm)) %>%
select(worm, contains('area')) %>%
mutate(worm = case_when(
worm == 'Y' ~ 'Single worm',
worm == 'N' ~ 'Debris',
worm == 'P' ~ 'Partial worm',
worm == 'M' ~ 'Multiple worms',
)) %>%
mutate(worm = as.factor(worm))
glimpse(annotations)
library(ggbeeswarm)
annotations %>%
pivot_longer(-worm, names_to = 'measurement', values_to = 'value') %>%
ggplot() +
geom_quasirandom(aes(x = worm, y = value, color = worm)) +
facet_wrap(facets = vars(measurement), scales = 'free_y') +
theme_minimal() +
NULL
model_data <- annotations %>%
mutate(worm = factor(worm))
set.seed(123)
# data_boot <- bootstraps(model_data, times = 2) # only 2 bootstraps for testing
data_split <- initial_split(model_data,
strata = worm)
train_data <- training(data_split)
test_data <- testing(data_split)
set.seed(234)
folds <- vfold_cv(train_data,
v = 10,
strata = worm)
parsnip:::parsnip_addin()
boost_tree_xgboost_spec <-
boost_tree(tree_depth = tune(), trees = tune(), learn_rate = tune(),
min_n = tune(), loss_reduction = tune(),
sample_size = tune(), stop_iter = tune()) %>%
set_engine('xgboost') %>%
set_mode('classification')
library(themis)
recipe <-
recipe(worm ~ ., data = model_data) %>%
step_nzv(all_predictors()) %>%
step_normalize(all_predictors()) %>%
step_corr(all_numeric_predictors(), threshold = .5) %>%
step_smote(worm)
prep <- prep(recipe)
juice <- juice(prep)
prep
glimpse(juice)
recipe2 <- recipe
recipe2$steps[[3]] <- update(recipe2$steps[[3]], skip = TRUE)
boost_workflow <-
workflow() %>%
add_model(boost_tree_xgboost_spec) %>%
add_recipe(recipe)
xg_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train_data),
learn_rate(),
size = 30
)
# tune on the train data
xg_tune <-
xg_workflow %>%
tune_grid(
resamples = folds,
grid = dt_grid,
control = control_grid(save_pred = TRUE,
verbose = TRUE),
metrics = metric_set(roc_auc, sens)
)
xg_workflow <-
workflow() %>%
add_model(boost_tree_xgboost_spec) %>%
add_recipe(recipe)
xg_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train_data),
learn_rate(),
size = 30
)
# tune on the train data
xg_tune <-
xg_workflow %>%
tune_grid(
resamples = folds,
grid = dt_grid,
control = control_grid(save_pred = TRUE,
verbose = TRUE),
metrics = metric_set(roc_auc, sens)
)
# tune on the train data
xg_tune <-
xg_workflow %>%
tune_grid(
resamples = folds,
grid = xg_grid,
control = control_grid(save_pred = TRUE,
verbose = TRUE),
metrics = metric_set(roc_auc, sens)
)
install.packages('xgboost')
doParallel::registerDoParallel()
# tune on the train data
xg_tune <-
xg_workflow %>%
tune_grid(
resamples = folds,
grid = xg_grid,
control = control_grid(save_pred = TRUE,
verbose = TRUE),
metrics = metric_set(roc_auc, sens)
)
boost_tree_xgboost_spec <-
boost_tree(tree_depth = tune(), trees = tune(), learn_rate = tune(),
min_n = tune(), loss_reduction = tune(), mtry = tune(),
sample_size = tune(), stop_iter = tune()) %>%
set_engine('xgboost') %>%
set_mode('classification')
xg_workflow <-
workflow() %>%
add_model(boost_tree_xgboost_spec) %>%
add_recipe(recipe)
xg_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train_data),
learn_rate(),
size = 30
)
doParallel::registerDoParallel()
# tune on the train data
xg_tune <-
xg_workflow %>%
tune_grid(
resamples = folds,
grid = xg_grid,
control = control_grid(save_pred = TRUE,
verbose = TRUE),
metrics = metric_set(roc_auc, sens)
)
xg_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train_data),
learn_rate(),
trees(),
stop_iter(),
size = 30
)
doParallel::registerDoParallel()
# tune on the train data
xg_tune <-
xg_workflow %>%
tune_grid(
resamples = folds,
grid = xg_grid,
control = control_grid(save_pred = TRUE,
verbose = TRUE),
metrics = metric_set(roc_auc, sens)
)
boost_tree_xgboost_spec <-
boost_tree(tree_depth = tune(), learn_rate = tune(),
min_n = tune(), loss_reduction = tune(), mtry = tune(),
sample_size = tune(), stop_iter = tune()) %>%
set_engine('xgboost') %>%
set_mode('classification')
xg_workflow <-
workflow() %>%
add_model(boost_tree_xgboost_spec) %>%
add_recipe(recipe)
xg_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train_data),
learn_rate(),
trees = 1000,
stop_iter(),
size = 30
)
xg_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train_data),
learn_rate(),
# trees = 1000,
stop_iter(),
size = 30
)
doParallel::registerDoParallel()
# tune on the train data
xg_tune <-
xg_workflow %>%
tune_grid(
resamples = folds,
grid = xg_grid,
control = control_grid(save_pred = TRUE,
verbose = TRUE),
metrics = metric_set(roc_auc, sens)
)
xg_tune <-
xg_workflow %>%
tune_grid(
resamples = folds,
grid = xg_grid,
control = control_grid(save_pred = TRUE,
verbose = TRUE),
metrics = metric_set(roc_auc, sens)
)
library(tidymodels)
library(tidyverse)
library(here)
files <- tibble(path = list.files(path = here(),
pattern = '.*_manual.csv',
recursive = TRUE))
get_data <- function(...) {
df <- tibble(...)
data <- read_csv(here(df$path))
}
annotations <- files %>%
pmap_dfr(get_data) %>%
janitor::clean_names() %>%
filter(!is.na(worm)) %>%
select(worm, contains('area')) %>%
mutate(worm = case_when(
worm == 'Y' ~ 'Single worm',
worm == 'N' ~ 'Debris',
worm == 'P' ~ 'Partial worm',
worm == 'M' ~ 'Multiple worms',
)) %>%
mutate(worm = as.factor(worm))
glimpse(annotations)
library(ggbeeswarm)
annotations %>%
pivot_longer(-worm, names_to = 'measurement', values_to = 'value') %>%
ggplot() +
geom_quasirandom(aes(x = worm, y = value, color = worm)) +
facet_wrap(facets = vars(measurement), scales = 'free_y') +
theme_minimal() +
NULL
model_data <- annotations %>%
mutate(worm = factor(worm))
set.seed(123)
# data_boot <- bootstraps(model_data, times = 2) # only 2 bootstraps for testing
data_split <- initial_split(model_data,
strata = worm)
train_data <- training(data_split)
test_data <- testing(data_split)
set.seed(234)
folds <- vfold_cv(train_data,
v = 10,
strata = worm)
decision_tree_rpart_spec <-
decision_tree(tree_depth = tune(), min_n = tune(), cost_complexity = tune()) %>%
set_engine('rpart') %>%
set_mode('classification')
multinom_reg_glmnet_spec <-
multinom_reg(penalty = tune(), mixture = tune()) %>%
set_engine('glmnet')
cores <- parallel::detectCores()
rand_forest_ranger_spec <-
rand_forest(mtry = tune(), min_n = tune()) %>%
set_engine('ranger', num.threads = cores) %>%
set_mode('classification')
svm_poly_kernlab_spec <-
svm_poly(cost = tune(), degree = tune(), scale_factor = tune(), margin = tune()) %>%
set_engine('kernlab') %>%
set_mode('classification')
boost_tree_xgboost_spec <-
boost_tree(tree_depth = tune(), learn_rate = tune(),
min_n = tune(), loss_reduction = tune(), mtry = tune(),
sample_size = tune(), stop_iter = tune()) %>%
set_engine('xgboost') %>%
set_mode('classification')
boost_tree_xgboost_spec <-
boost_tree(
trees = 1000,
tree_depth = tune(), min_n = tune(),
loss_reduction = tune(),
sample_size = tune(), mtry = tune(),
learn_rate = tune()) %>%
set_engine("xgboost") %>%
set_mode("classification")
library(themis)
recipe <-
recipe(worm ~ ., data = model_data) %>%
step_nzv(all_predictors()) %>%
step_normalize(all_predictors()) %>%
step_corr(all_numeric_predictors(), threshold = .5) %>%
step_smote(worm)
prep <- prep(recipe)
juice <- juice(prep)
prep
glimpse(juice)
recipe2 <- recipe
recipe2$steps[[3]] <- update(recipe2$steps[[3]], skip = TRUE)
dt_workflow <-
workflow() %>%
add_model(decision_tree_rpart_spec) %>%
add_recipe(recipe2)
mn_workflow <-
workflow() %>%
add_model(multinom_reg_glmnet_spec) %>%
add_recipe(recipe)
rf_workflow <-
workflow() %>%
add_model(rand_forest_ranger_spec) %>%
add_recipe(recipe2)
svm_poly_workflow <-
workflow() %>%
add_model(svm_poly_kernlab_spec) %>%
add_recipe(recipe)
xg_workflow <-
workflow() %>%
add_model(boost_tree_xgboost_spec) %>%
add_recipe(recipe)
xg_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train_data),
learn_rate(),
size = 30
)
xg_grid
# tune on the train data
xg_tune <-
xg_workflow %>%
tune_grid(
resamples = folds,
grid = xg_grid,
control = control_grid(save_pred = TRUE,
verbose = TRUE),
metrics = metric_set(roc_auc, sens)
)
write_rds(xg_tune, here('code', 'rds', 'xg_tune.rds'))
# extract the best decision tree
best_tree <- xg_tune %>%
select_best("roc_auc")
best_tree
# extract the best decision tree
best_xg <- xg_tune %>%
select_best("roc_auc")
# finalize the wf with the best tree
xg_workflow <-
xg_workflow %>%
finalize_workflow(best_xg)
# generate predictions on the hold-out test data
xg_auc <-
xg_tune %>%
collect_predictions(parameters = best_xg) %>%
roc_curve(.pred_Debris:`.pred_Single worm`, truth = worm) %>%
mutate(model = "XGBoost")
xg_auc
xg_auc %>%
autoplot()
